{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5536e298",
   "metadata": {},
   "source": [
    "Data Collection \n",
    "\n",
    "In this notebook we will collect the customer review data from the airline quality website called Skytrax and analyse it. Very helpful source to how to web scraping is the ducument in Medium, the link is here https://medium.com/analytics-vidhya/webscraping-a-site-with-pagination-using-beautifulsoup-fa0a09804445."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d35b0683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28a6e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the base URL and query parameters\n",
    "base_url='https://www.airlinequality.com/airline-reviews/british-airways'\n",
    "#query_parameter=\"/page/\"+str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afff5e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat an empty list to store all scraped reviews\n",
    "all_pages_reviews=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5b185ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a scraper function\n",
    "def scraper():\n",
    "    for i in range(1,10):\n",
    "        #creat an empty list to store the reviews of each page\n",
    "        pagewise_reviews=[]\n",
    "        #construct the URL\n",
    "        query_parameter=\"/page/\"+str(i)\n",
    "        url=base_url+query_parameter\n",
    "        #send http request to the URL using request and store response\n",
    "        response=requests.get(url)\n",
    "        #creat the soup object and parse the html\n",
    "        soup=BeautifulSoup(response.content,'html.parser')\n",
    "        #find all the div elements of class name text_content and store them in a variable\n",
    "        rev_div=soup.findAll(\"div\",attrs={\"class\",\"text_content\"})\n",
    "    #loop through all the rev_div and append the review text to the pagewise_reviews list\n",
    "    for j in range(len(rev_div)):\n",
    "        # finding all the strong tags to fetch only the review text\n",
    "        pagewise_reviews.append(rev_div[j].find('strong').text)\n",
    "        #append all page wise review to a single list 'all_pages_reviews'\n",
    "        for k in range(len(pagewise_reviews)):\n",
    "            all_pages_reviews.append(pagewise_reviews[k])\n",
    "            return all_pages_reviews\n",
    "# Call the function scraper() and store the output to a variable 'reviews' \n",
    "reviews = scraper()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa3e5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = range(1, len(reviews)+1)\n",
    "reviews_df = pd.DataFrame({'review':reviews}, index=i)\n",
    "df=reviews_df.to_csv('reviews.txt', sep='t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a99fc5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c25e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the html data\n",
    "page=requests.get(base_url)\n",
    "htmlData=page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c67d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pars the html data\n",
    "soup=BeautifulSoup(htmlData,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f5c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#organise the data with prettify\n",
    "print(soup.prettify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0820c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the title\n",
    "title=soup.title\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a8cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the text\n",
    "text=soup.get_text()\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f52014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull the titles and hrefs of all 10 reviews in first page\n",
    "reviews_rating_title=soup.find_all('div', class_=\"review-stats\")\n",
    "print(reviews_rating_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a8b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30c7fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=df['reviews_rating_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc443cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9c7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "paginated_url = f\"{url}/page/{page_num}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
    "print(paginated_url )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5c8f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "con=article.table.td.text\n",
    "print(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd058b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
